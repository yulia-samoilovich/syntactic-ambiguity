{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fb8b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d40ae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbe84d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "open.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be90761f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Cue Prompt:The employees knew the contract \n",
      "\n",
      "Generated Text:inside and out, as they had spent hours reviewing and discussing its terms and conditions\n",
      "\n",
      "No Cue Prompt:The mechanic accepted the car \n",
      "\n",
      "Generated Text:into the shop for repairs\n",
      "\n",
      "No Cue Prompt:The old man recalled the nurse\n",
      "\n",
      "Generated Text: who had cared for him during his recent hospital stay\n",
      "\n",
      "No Cue Prompt:The traveller heard the clock \n",
      "\n",
      "Generated Text:ticking in the distance, its steady rhythm echoing through the empty halls of the old mansion\n",
      "\n",
      "No Cue Prompt:The journalist confirmed the story \n",
      "\n",
      "Generated Text:The journalist verified the information in the story\n",
      "\n",
      "No Cue Prompt:The worker maintained the walls\n",
      "\n",
      "Generated Text:by regularly inspecting them for any signs of damage, such as cracks or water leaks, and promptly repairing any issues that were found\n",
      "\n",
      "No Cue Prompt:The apprentice forgot the bicycle \n",
      "\n",
      "Generated Text:at the store where he had borrowed it for a quick errand\n",
      "\n",
      "No Cue Prompt:The committee mentioned the issue\n",
      "\n",
      "Generated Text: at their last meeting and decided to address it in their next agenda\n",
      "\n",
      "No Cue Prompt:The army found the supplies\n",
      "\n",
      "Generated Text: they had been searching for in a hidden bunker deep in the forest\n",
      "\n",
      "No Cue Prompt:The umpire warned the spectators\n",
      "\n",
      "Generated Text:to refrain from making any disruptive or disrespectful comments towards the players on the field\n",
      "\n",
      "No Cue Prompt:The coach discovered the player\n",
      "\n",
      "Generated Text:during a local soccer match\n",
      "\n",
      "No Cue Prompt:The woman noticed the flyer\n",
      "\n",
      "Generated Text: on the bulletin board as she walked by\n",
      "\n",
      "No Cue Prompt:The tourists saw the palace \n",
      "\n",
      "Generated Text:and were amazed by its grandeur and beauty\n",
      "\n",
      "No Cue Prompt:The scientists proved the theory\n",
      "\n",
      "Generated Text: through a series of rigorous experiments and observations, providing concrete evidence to support their hypothesis\n",
      "\n",
      "No Cue Prompt:The soldiers remembered the town \n",
      "\n",
      "Generated Text:they had passed through on their way to battle\n",
      "\n",
      "No Cue Prompt:The priest recognized two guests \n",
      "\n",
      "Generated Text:who had just entered the church\n",
      "\n",
      "No Cue Prompt:The reporter revealed the politician \n",
      "\n",
      "Generated Text:'s involvement in a corruption scandal, detailing how he had accepted bribes in exchange for political favors\n",
      "\n",
      "No Cue Prompt:The owners insured the house\n",
      "\n",
      "Generated Text: against fire, theft, and other potential damages\n",
      "\n",
      "No Cue Prompt:The lawyer established the alibi \n",
      "\n",
      "Generated Text:for her client by presenting evidence that he was out of town at the time of the crime\n",
      "\n",
      "No Cue Prompt:The store guaranteed the television\n",
      "Generated Text: for one year against any defects or malfunctions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def complete_prompt(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.5,\n",
    "        max_tokens=60,\n",
    "        stop=[\".\"],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "with open(\"dataset.txt\") as phrases_file, open(\"no_cue_prompts.txt\") as prompts_file:\n",
    "    phrases = phrases_file.readlines()\n",
    "    prompts = prompts_file.readlines()\n",
    "    complete_sentences = []\n",
    "        \n",
    "    for prompt in prompts: \n",
    "        generated_text = complete_prompt(prompt)\n",
    "        complete_sentences.append(prompt + generated_text)\n",
    "        print(f\"No Cue Prompt:{prompt}\")\n",
    "        print(f\"Generated Text:{generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a98324a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2fddf51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"np.txt\") as np_sentences, open (\"s.txt\") as s_sentences:\n",
    "    np_sentences = np_sentences.read().lower()\n",
    "    s_sentences = s_sentences.read().lower()\n",
    "    \n",
    "    np_tokens = nltk.word_tokenize(np_sentences)\n",
    "    fdist_np = FreqDist(np_tokens)\n",
    "    re_pattern = r'\\w+'\n",
    "    np_clean = []\n",
    "    \n",
    "    s_tokens = nltk.word_tokenize(s_sentences)\n",
    "    fdist_s = FreqDist(s_tokens)\n",
    "    re_pattern = r'\\w+'\n",
    "    s_clean = []\n",
    "    \n",
    "    for word in np_tokens:\n",
    "        if re.match(re_pattern, word):\n",
    "            np_clean.append(word)\n",
    "            \n",
    "    for word in s_tokens:\n",
    "        if re.match(re_pattern, word):\n",
    "            s_clean.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "231a0f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'old', 'man', 'recalled', 'the', 'nurse', 'who', 'had', 'cared', 'for', 'him', 'during', 'his', 'recovery', 'at', 'the', 'hospital', 'the', 'worker', 'maintained', 'the', 'walls', 'by', 'regularly', 'inspecting', 'for', 'cracks', 'or', 'damage', 'patching', 'up', 'any', 'areas', 'that', 'needed', 'repair', 'and', 'repainting', 'or', 'sealing', 'as', 'necessary', 'the', 'apprentice', 'forgot', 'the', 'bicycle', 'at', 'the', 'shop', 'and', 'had', 'to', 'walk', 'home', 'instead', 'the', 'army', 'found', 'the', 'supplies', 'they', 'had', 'been', 'searching', 'for', 'in', 'a', 'hidden', 'bunker', 'deep', 'in', 'the', 'forest', 'the', 'umpire', 'warned', 'the', 'spectators', 'to', 'refrain', 'from', 'using', 'any', 'inappropriate', 'language', 'or', 'behavior', 'during', 'the', 'game', 'the', 'tourists', 'saw', 'the', 'palace', 'and', 'were', 'amazed', 'by', 'its', 'grandeur', 'and', 'beauty', 'the', 'scientists', 'proved', 'the', 'theory', 'through', 'extensive', 'research', 'and', 'experimentation', 'the', 'priest', 'recognized', 'two', 'guests', 'who', 'had', 'been', 'coming', 'to', 'the', 'church', 'for', 'years', 'the', 'reporter', 'exposed', 'the', 'politician', 'involvement', 'in', 'a', 'corruption', 'scandal', 'the', 'owners', 'insured', 'the', 'house', 'against', 'fire', 'theft', 'and', 'other', 'potential', 'damages', 'the', 'lawyer', 'established', 'the', 'alibi', 'for', 'his', 'client', 'by', 'providing', 'evidence', 'and', 'witnesses', 'to', 'prove', 'that', 'the', 'client', 'was', 'elsewhere', 'at', 'the', 'time', 'of', 'the', 'crime', 'the', 'store', 'guaranteed', 'the', 'television', 'for', 'one', 'year', 'against', 'any', 'defects', 'or', 'malfunctions', 'the', 'woman', 'noticed', 'the', 'flyer', 'taped', 'to', 'the', 'lamppost', 'as', 'she', 'walked', 'down', 'the', 'street', 'the', 'coach', 'discovered', 'the', 'player', 'during', 'a', 'local', 'soccer', 'match', 'the', 'journalist', 'verified', 'the', 'accuracy', 'of', 'the', 'story']\n"
     ]
    }
   ],
   "source": [
    "print(np_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f9296206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n"
     ]
    }
   ],
   "source": [
    "print(len(np_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "54c3f3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'employees', 'knew', 'the', 'contract', 'inside', 'and', 'out', 'as', 'they', 'had', 'spent', 'weeks', 'studying', 'and', 'reviewing', 'its', 'contents', 'the', 'mechanic', 'accepted', 'the', 'car', 'into', 'the', 'shop', 'for', 'repairs', 'the', 'traveller', 'heard', 'the', 'clock', 'ticking', 'in', 'the', 'distance', 'its', 'steady', 'rhythm', 'echoing', 'through', 'the', 'empty', 'streets', 'the', 'committee', 'mentioned', 'the', 'issue', 'the', 'committee', 'mentioned', 'the', 'issue', 'of', 'declining', 'attendance', 'at', 'community', 'events', 'and', 'discussed', 'potential', 'strategies', 'to', 'increase', 'participation', 'the', 'soldiers', 'remembered', 'the', 'town', 'they', 'had', 'liberated', 'just', 'a', 'few', 'days', 'before']\n"
     ]
    }
   ],
   "source": [
    "print(s_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "41402e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "print(len(s_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "89add7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 40),\n",
       " ('and', 7),\n",
       " ('for', 6),\n",
       " ('to', 5),\n",
       " ('had', 4),\n",
       " ('or', 4),\n",
       " ('during', 3),\n",
       " ('at', 3),\n",
       " ('by', 3),\n",
       " ('any', 3)]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_np = FreqDist(np_clean)\n",
    "fdist_np.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5745ca13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 15),\n",
       " ('and', 3),\n",
       " ('they', 2),\n",
       " ('had', 2),\n",
       " ('its', 2),\n",
       " ('committee', 2),\n",
       " ('mentioned', 2),\n",
       " ('issue', 2),\n",
       " ('employees', 1),\n",
       " ('knew', 1)]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_s = FreqDist(s_clean)\n",
    "fdist_s.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4138cd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The employees knew the contract \\ninside and out, as they had spent hours reviewing and discussing its terms and conditions',\n",
       " 'The mechanic accepted the car \\ninto the shop for repairs',\n",
       " 'The old man recalled the nurse\\n who had cared for him during his recent hospital stay',\n",
       " 'The traveller heard the clock \\nticking in the distance, its steady rhythm echoing through the empty halls of the old mansion',\n",
       " 'The journalist confirmed the story \\nThe journalist verified the information in the story',\n",
       " 'The worker maintained the walls\\nby regularly inspecting them for any signs of damage, such as cracks or water leaks, and promptly repairing any issues that were found',\n",
       " 'The apprentice forgot the bicycle \\nat the store where he had borrowed it for a quick errand',\n",
       " 'The committee mentioned the issue\\n at their last meeting and decided to address it in their next agenda',\n",
       " 'The army found the supplies\\n they had been searching for in a hidden bunker deep in the forest',\n",
       " 'The umpire warned the spectators\\nto refrain from making any disruptive or disrespectful comments towards the players on the field',\n",
       " 'The coach discovered the player\\nduring a local soccer match',\n",
       " 'The woman noticed the flyer\\n on the bulletin board as she walked by',\n",
       " 'The tourists saw the palace \\nand were amazed by its grandeur and beauty',\n",
       " 'The scientists proved the theory\\n through a series of rigorous experiments and observations, providing concrete evidence to support their hypothesis',\n",
       " 'The soldiers remembered the town \\nthey had passed through on their way to battle',\n",
       " 'The priest recognized two guests \\nwho had just entered the church',\n",
       " \"The reporter revealed the politician \\n's involvement in a corruption scandal, detailing how he had accepted bribes in exchange for political favors\",\n",
       " 'The owners insured the house\\n against fire, theft, and other potential damages',\n",
       " 'The lawyer established the alibi \\nfor her client by presenting evidence that he was out of town at the time of the crime',\n",
       " 'The store guaranteed the television for one year against any defects or malfunctions']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04e8efe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The employees knew the contract inside and out, as they had spent hours reviewing and discussing its terms and conditions',\n",
       " 'The mechanic accepted the car into the shop for repairs',\n",
       " 'The old man recalled the nurse who had cared for him during his recent hospital stay',\n",
       " 'The traveller heard the clock ticking in the distance, its steady rhythm echoing through the empty halls of the old mansion',\n",
       " 'The journalist confirmed the story The journalist verified the information in the story',\n",
       " 'The worker maintained the wallsby regularly inspecting them for any signs of damage, such as cracks or water leaks, and promptly repairing any issues that were found',\n",
       " 'The apprentice forgot the bicycle at the store where he had borrowed it for a quick errand',\n",
       " 'The committee mentioned the issue at their last meeting and decided to address it in their next agenda',\n",
       " 'The army found the supplies they had been searching for in a hidden bunker deep in the forest',\n",
       " 'The umpire warned the spectatorsto refrain from making any disruptive or disrespectful comments towards the players on the field',\n",
       " 'The coach discovered the playerduring a local soccer match',\n",
       " 'The woman noticed the flyer on the bulletin board as she walked by',\n",
       " 'The tourists saw the palace and were amazed by its grandeur and beauty',\n",
       " 'The scientists proved the theory through a series of rigorous experiments and observations, providing concrete evidence to support their hypothesis',\n",
       " 'The soldiers remembered the town they had passed through on their way to battle',\n",
       " 'The priest recognized two guests who had just entered the church',\n",
       " \"The reporter revealed the politician 's involvement in a corruption scandal, detailing how he had accepted bribes in exchange for political favors\",\n",
       " 'The owners insured the house against fire, theft, and other potential damages',\n",
       " 'The lawyer established the alibi for her client by presenting evidence that he was out of town at the time of the crime',\n",
       " 'The store guaranteed the television for one year against any defects or malfunctions']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_sentences = [sentence.replace('\\n', '') for sentence in complete_sentences]\n",
    "cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e353f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_sentences(messages,\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                temperature=0,\n",
    "                max_tokens=750):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "359bd3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ',\\n'.join([\n",
    "    f'{{\"sentence\": \"{sentence}\", \"is_NP\": true/false, \"is_S\": true/false}}'\n",
    "    for sentence in cleaned_sentences\n",
    "])\n",
    "\n",
    "system_message = \"\"\"\n",
    "You will be provided with a list of sentences.\n",
    "For each sentence, classify whether it is a noun phrase (NP) or a sentential complement (S).\n",
    "NP acts as the direct object of the main verb, while S acts as the embedded subject in an upcoming subordinate clause.\n",
    "Provide the output in JSON format with the following structure:\n",
    "\n",
    "{\n",
    "    \"sentences\": [\n",
    "        \"\"\" + sentences + \"\"\"\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': system_message}\n",
    "]\n",
    "\n",
    "response = categorize_sentences(messages)\n",
    "with open(\"output.json\", \"w\") as file:\n",
    "    file.write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e516392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"sentences\": [\n",
      "        {\"sentence\": \"The employees knew the contract inside and out, as they had spent hours reviewing and discussing its terms and conditions\", \"is_NP\": false, \"is_S\": true},\n",
      "        {\"sentence\": \"The mechanic accepted the car into the shop for repairs\", \"is_NP\": true, \"is_S\": false},\n",
      "        {\"sentence\": \"The old man recalled the nurse who had cared for him during his recent hospital stay\", \"is_NP\": true, \"is_S\": false},\n",
      "        {\"sentence\": \"The traveller heard the clock ticking in the distance, its steady rhythm echoing through the empty halls of the old mansion\", \"is_NP\": false, \"is_S\": true},\n",
      "        {\"sentence\": \"The journalist confirmed the story The journalist verified the information in the story\", \"is_NP\": true, \"is_S\": false},\n",
      "        {\"sentence\": \"The worker maintained the walls by regularly inspecting them for any signs of damage, such as cracks or water leaks, and promptly repairing any issues that were found\", \"is_NP\": true, \"is_S\": false},\n",
      "        {\"sentence\": \"The apprentice forgot the bicycle at the store where he had borrowed it for a quick errand\", \"is_NP\": true, \"is_S\": false},\n",
      "        {\"sentence\": \"The committee mentioned the issue at their last meeting and decided to address it in their next agenda\", \"is_NP\": true, \"is_S\": false},\n",
      "        {\"sentence\": \"The army found the supplies they had been searching for in a hidden bunker deep in the forest\", \"is_NP\": false, \"is_S\": true},\n",
      "        {\"sentence\": \"The umpire warned the spectators to refrain from making any disruptive or disrespectful comments towards the players on the field\", \"is_NP\": true, \"is_S\": false},\n",
      "        {\"sentence\": \"The coach discovered the player during a local soccer match\", \"is_NP\": true, \"is_S\": false},\n",
      "        {\"sentence\": \"The woman noticed the flyer on the bulletin board as she walked by\", \"is_NP\": true, \"is_S\": false},\n",
      "        {\"sentence\": \"The tourists saw the palace and were amazed by its grandeur and beauty\", \"is_NP\": true, \"is_S\": false},\n",
      "        {\"sentence\": \"The scientists proved the theory through a series of rigorous experiments and observations, providing concrete evidence to support their hypothesis\", \"is_NP\": true, \"is_S\": false},\n",
      "        {\"sentence\": \"The soldiers remembered the town they had passed through on their way to battle\", \"is_NP\": true, \"is_S\": false},\n",
      "        {\"sentence\": \"The priest recognized two guests who had just entered the church\", \"is_NP\": true, \"is_S\": false},\n",
      "        {\"sentence\": \"The reporter revealed the politician's involvement in a corruption scandal, detailing how he had accepted bribes in exchange for political favors\", \"is_NP\": false, \"is_S\": true},\n",
      "        {\"sentence\": \"The owners insured the house against fire, theft, and other potential damages\", \"is_NP\": true, \"is_S\": false},\n",
      "        {\"sentence\": \"The lawyer established the alibi for her client by presenting evidence that he was out of town at the time of the crime\", \"is_NP\": true, \"is_S\": false},\n",
      "        {\"sentence\": \"The store guaranteed the television for one year against any defects or malfunctions\", \"is_NP\": true, \"is_S\": false}\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "170ad6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "open.api_key = os.environ['ANTHROPIC_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25d417e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Completion(id='compl_01DfV2T5jX5WDPcXnt1P1Zz3', completion=' Hello!', model='claude-2.1', stop_reason='stop_sequence', type='completion', stop='\\n\\nHuman:', log_id='compl_01DfV2T5jX5WDPcXnt1P1Zz3')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import anthropic\n",
    "from anthropic import HUMAN_PROMPT, AI_PROMPT\n",
    "\n",
    "anthropic.Anthropic().completions.create(\n",
    "    model=\"claude-2.1\",\n",
    "    max_tokens_to_sample=70,\n",
    "    prompt=f\"{HUMAN_PROMPT} Hello, Claude{AI_PROMPT}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "33fbdccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "client = anthropic.Anthropic()\n",
    "\n",
    "with client.messages.stream(\n",
    "    max_tokens=1024,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "    model=\"claude-3-opus-20240229\",\n",
    ") as stream:\n",
    "  for text in stream.text_stream:\n",
    "      print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8a23681e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in two ways each:\n",
      "\n",
      "The employees knew the contractNone\n",
      " in two different ways:\n",
      "\n",
      "1) The mechanic accepted the carNone\n",
      " in a way that does not depict or encourage harmful or unethical actsNone\n",
      ":\n",
      "\n",
      "The traveller heard the clock strike midnight as he arrived at the old, abandoned mansionNone\n",
      " in two ways each - first, assume a positive sentiment and interpretationNone\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of requests has exceeded your rate limit (https://docs.anthropic.com/claude/reference/rate-limits). Please try again later or contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m complete_sentences_anthropic \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts: \n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(complete_prompt_anthropic(prompt\u001b[38;5;241m.\u001b[39mstrip()))\n",
      "Cell \u001b[0;32mIn[124], line 3\u001b[0m, in \u001b[0;36mcomplete_prompt_anthropic\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcomplete_prompt_anthropic\u001b[39m(prompt):\n\u001b[1;32m      2\u001b[0m     response \u001b[38;5;241m=\u001b[39m anthropic\u001b[38;5;241m.\u001b[39mAnthropic()\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m response\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m      4\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaude-3-opus-20240229\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m      6\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m,\n\u001b[1;32m      7\u001b[0m         stop_sequences\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      9\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt},\n\u001b[1;32m     10\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplete the ambiguous prompts below\u001b[39m\u001b[38;5;124m\"\u001b[39m}         \n\u001b[1;32m     11\u001b[0m         ]\n\u001b[1;32m     12\u001b[0m     )\u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mtext_stream:\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;28mprint\u001b[39m(text, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/anthropic/lib/streaming/_messages.py:196\u001b[0m, in \u001b[0;36mMessageStreamManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MessageStreamT:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__api_request()\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__stream\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/anthropic/_base_client.py:1208\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1196\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1204\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1205\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1206\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1207\u001b[0m     )\n\u001b[0;32m-> 1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/anthropic/_base_client.py:897\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    890\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    895\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    898\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    899\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    900\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    901\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    902\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    903\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/anthropic/_base_client.py:973\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    972\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    974\u001b[0m         options,\n\u001b[1;32m    975\u001b[0m         cast_to,\n\u001b[1;32m    976\u001b[0m         retries,\n\u001b[1;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    978\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    979\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    980\u001b[0m     )\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/anthropic/_base_client.py:1021\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1022\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1025\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1026\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1027\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/anthropic/_base_client.py:973\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    972\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[1;32m    974\u001b[0m         options,\n\u001b[1;32m    975\u001b[0m         cast_to,\n\u001b[1;32m    976\u001b[0m         retries,\n\u001b[1;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    978\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    979\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    980\u001b[0m     )\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/anthropic/_base_client.py:1021\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1022\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[1;32m   1025\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1026\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1027\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/anthropic/_base_client.py:988\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    985\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    987\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    991\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    992\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    995\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    996\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of requests has exceeded your rate limit (https://docs.anthropic.com/claude/reference/rate-limits). Please try again later or contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}"
     ]
    }
   ],
   "source": [
    "def complete_prompt_anthropic(prompt):\n",
    "    response = anthropic.Anthropic()\n",
    "    with response.messages.stream(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        temperature=0.5,\n",
    "        max_tokens=150,\n",
    "        stop_sequences=[\".\"],\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": \"Complete the ambiguous prompts below\"}         \n",
    "        ]\n",
    "    )as stream:\n",
    "        for text in stream.text_stream:\n",
    "            print(text, end=\"\", flush=True)\n",
    "\n",
    "with open(\"no_cue_prompts.txt\") as prompts_file:\n",
    "    prompts = prompts_file.readlines()\n",
    "    complete_sentences_anthropic = []\n",
    "        \n",
    "    for prompt in prompts: \n",
    "        print(complete_prompt_anthropic(prompt.strip()))\n",
    "#         generated_text = complete_prompt_anthropic(prompt.strip())  \n",
    "#         #complete_sentences_anthropic.append(prompt.strip() + generated_text)  \n",
    "#         print(f\"No Cue Prompt:{prompt.strip()}\")\n",
    "#         print(f\"Generated Text:{generated_text}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "dd1e08cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'messages: first message must use the \"user\" role'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m complete_sentences_anthropic \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m---> 18\u001b[0m     complete_sentence \u001b[38;5;241m=\u001b[39m complete_prompt_anthropic(prompt\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m     19\u001b[0m     complete_sentences_anthropic\u001b[38;5;241m.\u001b[39mappend(complete_sentence)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(complete_sentence)\n",
      "Cell \u001b[0;32mIn[149], line 2\u001b[0m, in \u001b[0;36mcomplete_prompt_anthropic\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcomplete_prompt_anthropic\u001b[39m(prompt):\n\u001b[0;32m----> 2\u001b[0m     response \u001b[38;5;241m=\u001b[39m anthropic\u001b[38;5;241m.\u001b[39mAnthropic()\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      3\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaude-3-opus-20240229\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m,\n\u001b[1;32m      5\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      6\u001b[0m         stop_sequences\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      7\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      8\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplete the prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m      9\u001b[0m         ]\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/anthropic/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/anthropic/resources/messages.py:678\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m600\u001b[39m,\n\u001b[1;32m    677\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Message \u001b[38;5;241m|\u001b[39m Stream[MessageStreamEvent]:\n\u001b[0;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    679\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/v1/messages\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    680\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    681\u001b[0m             {\n\u001b[1;32m    682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop_sequences,\n\u001b[1;32m    687\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    688\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m: system,\n\u001b[1;32m    689\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    690\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_k,\n\u001b[1;32m    691\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    692\u001b[0m             },\n\u001b[1;32m    693\u001b[0m             message_create_params\u001b[38;5;241m.\u001b[39mMessageCreateParams,\n\u001b[1;32m    694\u001b[0m         ),\n\u001b[1;32m    695\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    696\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    697\u001b[0m         ),\n\u001b[1;32m    698\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mMessage,\n\u001b[1;32m    699\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    700\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[MessageStreamEvent],\n\u001b[1;32m    701\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/anthropic/_base_client.py:1208\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1196\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1204\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1205\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1206\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1207\u001b[0m     )\n\u001b[0;32m-> 1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/anthropic/_base_client.py:897\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    890\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    895\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    898\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    899\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    900\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    901\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    902\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[1;32m    903\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/anthropic/_base_client.py:988\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    985\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    987\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    991\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    992\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    995\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    996\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'messages: first message must use the \"user\" role'}}"
     ]
    }
   ],
   "source": [
    "def complete_prompt_anthropic(prompt):\n",
    "    response = anthropic.Anthropic().messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        temperature=0.6,\n",
    "        max_tokens=100,\n",
    "        stop_sequences=[\".\"],\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": \"Complete the prompt\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "with open(\"no_cue_prompts.txt\") as prompts_file:\n",
    "    prompts = prompts_file.readlines()\n",
    "    complete_sentences_anthropic = []\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        complete_sentence = complete_prompt_anthropic(prompt.strip())\n",
    "        complete_sentences_anthropic.append(complete_sentence)\n",
    "        print(complete_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f69d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
